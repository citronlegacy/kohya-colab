{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WIP Pinterest Dataset Maker by Citron Legacy 🍋\n",
        "\n",
        "Help fuel my passion!\n",
        " [![ko-fi](https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/citronlegacy)\n",
        "\n",
        "Any amount would be awesome!\n",
        "\n",
        "![](https://i.imgur.com/sjXiQwT.png)\n",
        "\n",
        "\n",
        "### Project Description\n",
        "\n",
        "This project is for simplying the training of Loras for Stable Diffusion. Use it to train an already captioned dataset in Google Drive.\n",
        "\n",
        "There are a lot of great Lora training tools with nice features but this one is intended to hide advanced settings and make the simplest trainer possible.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Links\n",
        "| Project |GitHub| Colab | | Other content | Link|\n",
        "|:--|:-:|:-:|:-:|:--|:--|\n",
        "| 🏠 **Homepage** | [![GitHub](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/github.svg)](https://github.com/citronlegacy/kohya-colab) | | | ☕ **Ko-fi** | [![Ko-Fi](https://img.shields.io/badge/Ko--Fi-Support-orange.svg)](https://ko-fi.com/citronlegacy) |\n",
        "| 📊 **Citron Dataset Maker** | [![GitHub](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/github.svg)](https://github.com/citronlegacy/kohya-colab/blob/main/Citron_Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/citronlegacy/kohya-colab/blob/main/Citron_Dataset_Maker.ipynb) | |🤖 **CivitAI** | [![CivitAI](https://img.shields.io/badge/CivitAI-Models-blue.svg)](https://civitai.com/user/CitronLegacy/models) |\n",
        "| 💪 **Citron Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/github.svg)](https://github.com/citronlegacy/kohya-colab/blob/main/Citron_Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/citronlegacy/kohya-colab/blob/main/Citron_Lora_Trainer.ipynb) | | 🎨 **Pixiv** | [![Pixiv](https://img.shields.io/badge/Pixiv-Profile-purple.svg)](https://www.pixiv.net/en/users/95364318) |\n",
        "| ⭐ **Coming soon!! CALM - Citron Auto Lora Maker** |  |  | | 🎬 **Youtube**  | [![YouTube](https://img.shields.io/badge/YouTube-Subscribe-red.svg)](https://www.youtube.com/@FujiwaraNoMokou11) |\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "### Project Disclaimer\n",
        "This is forked from the work of [Hollowstrawberry 🍓](https://github.com/hollowstrawberry/kohya-colab) which is based on the work of [Kohya-ss](https://github.com/kohya-ss/sd-scripts) and [Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb). Thank you!\n",
        "\n",
        "Please read and follow the [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) and its [Terms of Service](https://research.google.com/colaboratory/tos_v3.html).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HCOeLF9-2CbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Install\n",
        "#@markdown ### 1️⃣ Setup Connect to Google Drive and Install Dependences\n",
        "#@markdown Installation usually takes about 3 minutes\n",
        "\n",
        "#@markdown ------------------------------------------------------\n",
        "\n",
        "#@markdown Update 02/02/2024 - Started Project\n",
        "\n",
        "import os\n",
        "import time\n",
        "dependencies_installed = False\n",
        "googleDrive_start_time = time.perf_counter()\n",
        "\n",
        "from google.colab.output import clear as clear_output\n",
        "from google.colab import drive\n",
        "print(\"📂 Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "googleDrive_end_time = time.perf_counter()\n",
        "\n",
        "\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "COLAB = True # low ram\n",
        "COMMIT = \"e6ad3cbc66130fdc3bf9ecd1e0272969b1d613f7\"\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "\"\"\"\n",
        "if \"step4a_installed_flag\" not in globals():\n",
        "    print(\"\\n🏭 Installing dependencies...\\n\")\n",
        "\n",
        "    #!pip -q install tensorflow==2.12.0 huggingface-hub==0.12.0 accelerate==0.15.0 transformers==4.26.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6 torchvision albumentations\n",
        "    !pip -q install -U tensorflow huggingface-hub==0.12.0 accelerate==0.15.0 transformers==4.26.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6 torchvision albumentations\n",
        "    #!pip install xformers==0.0.22.post7\n",
        "    !apt -y install aria2\n",
        "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "      clear_output()\n",
        "      step4a_installed_flag = True\n",
        "    else:\n",
        "      print(\"❌ Error installing dependencies, trying to continue anyway...\")\n",
        "\n",
        "def clone_repo():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone https://github.com/kohya-ss/sd-scripts {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/requirements.txt -q -O requirements.txt\n",
        "\n",
        "aiInstalls_end_time = time.perf_counter()\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def clone_repo():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone https://github.com/kohya-ss/sd-scripts {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/requirements.txt -q -O requirements.txt\n",
        "\n",
        "def install_dependencies():\n",
        "  clone_repo()\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "  !pip install --upgrade -r requirements.txt\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if COLAB:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py # low ram\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "  global dependencies_installed\n",
        "  dependencies_installed = True\n",
        "\n",
        "aiInstalls_start_time = time.perf_counter()\n",
        "install_dependencies()\n",
        "aiInstalls_end_time = time.perf_counter()\n",
        "\n",
        "\n",
        "#######################################################\n",
        "##### Start - Define Citron's Library Fuctions\n",
        "#######################################################\n",
        "\n",
        "def writeToFile(filename, text):\n",
        "  ! echo {text} >> {filename}\n",
        "  #! cat {filename}\n",
        "\n",
        "def clearFile(filename):\n",
        "   ! echo \"\" > {filename}\n",
        "\n",
        "def writeLineToFile(filename):\n",
        "  ! echo \"==============================\" >> {filename}\n",
        "\n",
        "#######################################################\n",
        "##### End - Define Citron's Library Fuctions\n",
        "#######################################################\n",
        "\n",
        "#Import colabUtilities\n",
        "!git clone https://github.com/citronlegacy/kohya-colab.git\n",
        "# CD into project\n",
        "%cd kohya-colab\n",
        "# import modules\n",
        "import colabUtilities\n",
        "#return to original directory\n",
        "%cd ..\n",
        "\n",
        "print(\"Google drive setup time: \" + str(colabUtilities.get_time_hh_mm_ss(googleDrive_end_time-googleDrive_start_time)) + \" minutes\")\n",
        "print(\"AI Installation time: \" + str(colabUtilities.get_time_hh_mm_ss(aiInstalls_end_time-aiInstalls_start_time)) + \" minutes\")"
      ],
      "metadata": {
        "id": "x_xaxQL57ImS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pinterest Downloader\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def download_pinterest_images(board_url, images_folder):\n",
        "    # Send an HTTP GET request to the board URL\n",
        "    response = requests.get(board_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all image tags\n",
        "        img_tags = soup.find_all('img')\n",
        "\n",
        "        # Create a folder to save the images inside the specified images_folder\n",
        "        save_folder = images_folder\n",
        "        os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "        # Download each image\n",
        "        print(f\"Found {len(img_tags)} images\")\n",
        "        for img_tag in img_tags:\n",
        "            img_url = img_tag['src']\n",
        "            img_name = img_url.split('/')[-1]\n",
        "            img_path = os.path.join(save_folder, img_name)\n",
        "\n",
        "            with open(img_path, 'wb') as img_file:\n",
        "                img_data = requests.get(img_url).content\n",
        "                img_file.write(img_data)\n",
        "\n",
        "            print(f\"Downloaded: {img_name}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "project_name = \"test\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "\n",
        "url_to_pinterest_board = \"https://www.pinterest.com/citronlegacy/github-octocat/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Folder Structure is Organized by project name: MyDrive/lora_training/datasets/project_name\n",
        "folder_structure = \"Organize by category (MyDrive/lora_training/datasets/project_name)\"\n",
        "\n",
        "main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "\n",
        "# Replace the URL with the desired Pinterest board URL\n",
        "download_pinterest_images(url_to_pinterest_board, images_folder)\n",
        "\n"
      ],
      "metadata": {
        "id": "ki-Q7258LLSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nBUO636P85t",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display, Markdown\n",
        "import json\n",
        "from urllib.request import urlopen, Request\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "COLAB = True\n",
        "\n",
        "if COLAB:\n",
        "  from google.colab.output import clear as clear_output\n",
        "else:\n",
        "  from IPython.display import clear_output\n",
        "\n",
        "#defining variables at the beginning to fix a bug with the log file - lol such programming\n",
        "remove_tags = \"NA - Tagging Skipped\"\n",
        "topTags = \"NA - Tagging Skipped\"\n",
        "total_steps = 0 #Defining this variable here so that its a global varaible\n",
        "\n",
        "\n",
        "#@title # Dataset Maker\n",
        "#@markdown 💡 Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): Dataset creation is the most important part of Lora training. Take your time and have fun collecting images of something you like.\n",
        "#@markdown This project has a lot of tips but feel free to ignore them! You are the creator, don't let anything restrict your creativity! 🎉\n",
        "#@markdown ### 1️⃣ Setup\n",
        "download_images = True #@param {type:\"boolean\"}\n",
        "tag_images = True #@param {type:\"boolean\"}\n",
        "create_logs = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Your project name can't contain spaces\n",
        "project_name = \"Hatsune_Miku\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "#@markdown Folder Structure is Organized by project name: MyDrive/lora_training/datasets/project_name\n",
        "folder_structure = \"Organize by category (MyDrive/lora_training/datasets/project_name)\"\n",
        "\n",
        "if not project_name or any(c in project_name for c in \" .()\\\"'\\\\\") or project_name.count(\"/\") > 1:\n",
        "  print(\"Please write a valid project_name.\")\n",
        "\n",
        "project_base = project_name if \"/\" not in project_name else project_name[:project_name.rfind(\"/\")]\n",
        "project_subfolder = project_name if \"/\" not in project_name else project_name[project_name.rfind(\"/\")+1:]\n",
        "\n",
        "\n",
        "main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "\n",
        "for dir in [main_dir, deps_dir, images_folder, config_folder]:\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "print(f\"✅ Project {project_name} is ready!\")\n",
        "\n",
        "\n",
        "#######################################################\n",
        "##### STEP - Image Downloading\n",
        "#######################################################\n",
        "print(\"#######################################################\")\n",
        "print(\"##### STEP - Image Downloading\")\n",
        "print(\"#######################################################\")\n",
        "\n",
        "if (not download_images):\n",
        "  print(\"skipping image download\")\n",
        "  gelbooruSearchQuery = \"NA step is skipped\" #setting this value because this step is skipped\n",
        "if (download_images):\n",
        "\n",
        "  #@markdown ### 2️⃣ Scrape images from Gelbooru\n",
        "\n",
        "  #@markdown We will grab images from the popular anime gallery [Gelbooru](https://gelbooru.com/). Images are sorted by tags, including poses, scenes, character traits, character names, artists, etc. <p>\n",
        "  #@markdown * If you instead want to download screencaps of anime episodes, try [this other colab by another person](https://colab.research.google.com/drive/1oBSntB40BKzNmKceXUlkXzujzdQw-Ci7). It's more complicated though.\n",
        "\n",
        "  #@markdown Up to 1000 images may be downloaded by this step in just one minute. Don't abuse it. <p>\n",
        "\n",
        "  #@markdown Note: putting a minus sign in front of a tag will exclude images with that tag from the search result. For example, `-pink_hair` will mean you won't get images with the `pink_hair` tag\n",
        "\n",
        "  #@markdown 💡 Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): If you want to be really detailed you can run download step several times with different search queries.\n",
        "  #@markdown For example, get a few images with the `from_behind` or `from_side` tag so that the Lora can learn those angles/concepts.\n",
        "\n",
        "  tags = \"Hatsune_Miku\" #@param {type:\"string\"}\n",
        "  #@markdown You don't have to use this but sometimes it nice to define tags that never change in this seperate input field. For example I always want search results sorted by score and I never want images with certain tags\n",
        "  extra_tags = \"sort:score solo -animated -crying -1boy -monochrome -duskyer -rakko_(r2) -slave -injury -scared -futanari -geebomb -bokuman -1340smile -osg_pk -2girls -among_us -rope -bdsm -feet \" #@param {type:\"string\"}\n",
        "\n",
        "  #@markdown the tag `rating:general` is basically the SFW tag on Gelbooru. You can use these these checkboxes to either limit results to SFW images or exclude SFW images from results.\n",
        "\n",
        "  #@markdown 💡 Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): I recommend having at least 75% SFW images if you want to train an outfit. Too many NSFW images may result in a lack of data about clothes.\n",
        "  apply_tag_rating_general = False #@param {type:\"boolean\"}\n",
        "  apply_tag_minus_rating_general = False #@param {type:\"boolean\"}\n",
        "  if (apply_tag_rating_general):\n",
        "    tags = \"rating:general \" + tags\n",
        "  if (apply_tag_minus_rating_general):\n",
        "     tags = \"-rating:general \" + tags\n",
        "\n",
        "  tags = tags + \" \" + extra_tags\n",
        "  gelbooruSearchQuery = tags\n",
        "  gelbooruSearchQuery = gelbooruSearchQuery.replace(\"(\", \"\\(\").replace(\")\", \"\\)\")\n",
        "\n",
        "  ##@markdown If an image is bigger than this resolution a smaller version will be downloaded instead.\n",
        "  max_resolution = 3072 #param {type:\"slider\", min:1024, max:8196, step:1024}\n",
        "  ##@markdown Posts with a parent post are often minor variations of the same image.\n",
        "  include_posts_with_parent = True #param {type:\"boolean\"}\n",
        "\n",
        "  tags = tags.replace(\" \", \"+\")\\\n",
        "            .replace(\"(\", \"%28\")\\\n",
        "            .replace(\")\", \"%29\")\\\n",
        "            .replace(\":\", \"%3a\")\\\n",
        "            .replace(\"&\", \"%26\")\\\n",
        "\n",
        "  url = \"https://gelbooru.com/index.php?page=dapi&json=1&s=post&q=index&limit=100&tags={}\".format(tags)\n",
        "  user_agent = \"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; Googlebot/2.1; +http://www.google.com/bot.html) Chrome/93.0.4577.83 Safari/537.36\"\n",
        "  limit = 100 # hardcoded by gelbooru\n",
        "  #@markdown Enter maximum number of images to download from Gelbooru (There is a bug where it sometimes downloads 1 more/less than the number entered)\n",
        "\n",
        "  #@markdown 💡 Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): You can make a decent Lora with 50 to 100 images. 300+ images is great but can take a really long time to train.\n",
        "  maxNumberOfImages = 50 #@param {type:\"number\"}\n",
        "  total_limit = maxNumberOfImages\n",
        "  # Testing setting the limit of images at this point in the code. If this works then the line where the url is set above can be deleted\n",
        "  #urlwithLimit = \"https://gelbooru.com/index.php?page=dapi&json=1&s=post&q=index&limit=\"+str(maxNumberOfImages)+\"&tags={}\".format(tags)\n",
        "  #url = urlwithLimit.format(tags)\n",
        "  #if (maxNumberOfImages < 100):\n",
        "  #  url = \"https://gelbooru.com/index.php?page=dapi&json=1&s=post&q=index&limit=\"+str(maxNumberOfImages)+\"&tags={}\".format(tags)\n",
        "\n",
        "  #url = \"https://gelbooru.com/index.php?page=dapi&json=1&s=post&q=index&limit=\"+str(maxNumberOfImages)+\"&tags={}\".format(tags)\n",
        "\n",
        "  print(url)\n",
        "\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\")\n",
        "\n",
        "  \"\"\"\n",
        "  def ubuntu_deps():\n",
        "    print(\"🏭 Installing dependencies...\\n\")\n",
        "    !apt -y install aria2\n",
        "    return not get_ipython().__dict__['user_ns']['_exit_code']\n",
        "\n",
        "  if \"step2_installed_flag\" not in globals():\n",
        "    if ubuntu_deps():\n",
        "      #clear_output()\n",
        "      step2_installed_flag = True\n",
        "    else:\n",
        "      print(\"❌ Error installing dependencies, attempting to continue anyway...\")\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def get_json(url):\n",
        "    print(\"get_json url = \" + url)\n",
        "    with urlopen(Request(url, headers={\"User-Agent\": user_agent})) as page:\n",
        "      return json.load(page)\n",
        "\n",
        "  def filter_images(data):\n",
        "    return [p[\"file_url\"] if p[\"width\"]*p[\"height\"] <= max_resolution**2 else p[\"sample_url\"]\n",
        "            for p in data[\"post\"]\n",
        "            if (p[\"parent_id\"] == 0 or include_posts_with_parent)\n",
        "            and p[\"file_url\"].lower().endswith(supported_types)]\n",
        "\n",
        "  def download_images():\n",
        "    count = 0\n",
        "    if(maxNumberOfImages < 100):\n",
        "      smallerThan100DownloadURL = \"https://gelbooru.com/index.php?page=dapi&json=1&s=post&q=index&limit=\"+str(maxNumberOfImages+1)+\"&tags={}\".format(tags)\n",
        "      data = get_json(smallerThan100DownloadURL)\n",
        "      count = data[\"@attributes\"][\"count\"]\n",
        "    else:\n",
        "      data = get_json(url)\n",
        "      count = data[\"@attributes\"][\"count\"]\n",
        "\n",
        "    if count == 0:\n",
        "      print(\"📷 No results found\")\n",
        "      return\n",
        "\n",
        "    print(f\"🎯 Found {count} results\")\n",
        "    test_url = \"https://gelbooru.com/index.php?page=post&s=list&tags={}\".format(tags)\n",
        "    display(Markdown(f\"[Click here to open in browser!]({test_url})\"))\n",
        "    print (f\"🔽 Will download to {images_folder.replace('/content/drive/', '')} (A confirmation box should appear below, otherwise run this cell again)\")\n",
        "    inp = 'yes' #input(\"❓ Enter the word 'yes' if you want to proceed with the download: \")\n",
        "\n",
        "    if inp.lower().strip() != 'yes':\n",
        "      print(\"❌ Download cancelled\")\n",
        "      return\n",
        "\n",
        "    print(\"📩 Grabbing image list...\")\n",
        "\n",
        "    image_urls = set()\n",
        "    image_urls = image_urls.union(filter_images(data))\n",
        "    for i in range(total_limit // limit):\n",
        "      print(\"inside loop\")\n",
        "      numberOfImagesDownloadLinksInFile = len(image_urls)\n",
        "      numberOfDownloadsRemaining = maxNumberOfImages - numberOfImagesDownloadLinksInFile\n",
        "      #Debugging log - can be deleted\n",
        "      print (f\"i = {i}; image_urls = {len(image_urls)}; total_limit = {total_limit}; limit = {limit}; numberOfImagesDownloadLinksInFile = {numberOfImagesDownloadLinksInFile}; numberOfDownloadsRemaining = {numberOfDownloadsRemaining} \" )\n",
        "      count -= limit\n",
        "      if count <= 0:\n",
        "        break\n",
        "      time.sleep(0.1)\n",
        "\n",
        "      # Reformat URLs to ensure only the correct amount of images is downloaded\n",
        "      #Added if-block to ensure that the last set of images to download doesnt go over the maxNumberOfImages\n",
        "      #Determine how many download links are remaining\n",
        "      #If there are less than the limit (hardcoded to 100) then only add the right amount of urls to the downloads list\n",
        "      numberOfDownloadsRemaining = maxNumberOfImages - numberOfImagesDownloadLinksInFile\n",
        "      if (numberOfDownloadsRemaining < 100):\n",
        "        filterImagesResult = filter_images(get_json(url+f\"&pid={i+1}\"))\n",
        "        limitedFilterImagesResult = filterImagesResult[0:numberOfDownloadsRemaining]\n",
        "        image_urls = image_urls.union(limitedFilterImagesResult)\n",
        "      else:\n",
        "        image_urls = image_urls.union(filter_images(get_json(url+f\"&pid={i+1}\")))\n",
        "\n",
        "    scrape_file = os.path.join(config_folder, f\"scrape_{project_subfolder}.txt\")\n",
        "    with open(scrape_file, \"w\") as f:\n",
        "      f.write(\"\\n\".join(image_urls))\n",
        "\n",
        "    print(f\"🌐 Saved links to {scrape_file}\\n\\n🔁 Downloading images...\\n\")\n",
        "    old_img_count = len([f for f in os.listdir(images_folder) if f.lower().endswith(supported_types)])\n",
        "\n",
        "    os.chdir(images_folder)\n",
        "    !aria2c --console-log-level=warn -c -x 16 -k 1M -s 16 -i {scrape_file}\n",
        "\n",
        "    new_img_count = len([f for f in os.listdir(images_folder) if f.lower().endswith(supported_types)])\n",
        "    print(f\"\\n✅ Downloaded {new_img_count - old_img_count} images.\")\n",
        "    print(f\"\\n number of images in image_urls: {len(image_urls)} \")\n",
        "\n",
        "  download_images()\n",
        "  #clear_output()\n",
        "\n",
        "#######################################################\n",
        "##### STEP - Tagging\n",
        "#######################################################\n",
        "print(\"#######################################################\")\n",
        "print(\"##### STEP - Tagging\")\n",
        "print(\"#######################################################\")\n",
        "\n",
        "#@markdown ### 3️⃣ Tag your images\n",
        "#@markdown We will be using AI to automatically tag your images, specifically [Waifu Diffusion](https://huggingface.co/SmilingWolf/wd-v1-4-swinv2-tagger-v2) in the case of anime and [BLIP](https://huggingface.co/spaces/Salesforce/BLIP) in the case of photos.\n",
        "#@markdown Giving tags/captions to your images allows for much better training. This process should take a couple minutes. <p>\n",
        "\n",
        "#@markdown ❗ Important: you can choose to not enter anything in this section if you want to train your lora without a trigger\n",
        "\n",
        "trigger = \"Hatsune_Miku\" #@param {type:\"string\"}\n",
        "global_activation_tag = trigger.strip()\n",
        "if (not tag_images):\n",
        "  print(\"skipping image tagging\")\n",
        "if (tag_images):\n",
        "  start_time_tagging = time.perf_counter()\n",
        "  method = \"Anime tags\"\n",
        "  #@markdown Abosrb tags that represent your Lora. This could be details like eye color or concepts like `glowing`\n",
        "\n",
        "  #@markdown 💡 Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): If you aren't sure what tags to absorb run this to see what tags are most common. You can check the logs and the rerun this with more absorbed tags\n",
        "\n",
        "  #@markdown 💡 Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): For Character Loras, I recommend absorbing `1girl` or `1boy`, `solo`, eye color, hair color, and hair length/style (`short_hair`, `long_hair`, `twintails`, etc)\n",
        "\n",
        "  #@markdown 💡 Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): For Character Loras, I recommend not absorbing details about an outfit. If you absorb the outfit into the trigger the Lora will not be flexible enough to change the outfit\n",
        "\n",
        "  absorbed_these_tags_into_trigger = \"1girl, solo, \" #@param {type:\"string\"}\n",
        "  #@markdown Change the tag threshold if you are not getting enough tags\n",
        "  tag_threshold = 0.4 #@param {type:\"number\"}\n",
        "  #@markdown These tags will be ignored and thus not added to the captions\n",
        "  blacklist_tags = \"bangs, breasts, multicolored hair, two-tone hair, gradient hair, virtual youtuber, official alternate costume, official alternate hairstyle, official alternate hair length, alternate costume, alternate hairstyle, alternate hair length, alternate hair color\" #@param {type:\"string\"}\n",
        "  blacklist_tags_2 = \"\"\n",
        "\n",
        "  numberOfTopTagsToAbsorb = 0\n",
        "\n",
        "  extraAbsorbedTags = absorbed_these_tags_into_trigger\n",
        "  caption_min = 15\n",
        "  caption_max = 75\n",
        "\n",
        "  %env PYTHONPATH=/env/python\n",
        "  os.chdir(root_dir)\n",
        "  kohya = \"/content/kohya-trainer\"\n",
        "  if not os.path.exists(kohya):\n",
        "    !git clone https://github.com/kohya-ss/sd-scripts {kohya}\n",
        "    os.chdir(kohya)\n",
        "    !git reset --hard 5050971ac687dca70ba0486a583d283e8ae324e2\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "  if \"tags\" in method:\n",
        "    \"\"\"\n",
        "    if \"step4a_installed_flag\" not in globals():\n",
        "      print(\"\\n🏭 Installing dependencies...\\n\")\n",
        "      #!pip -q install tensorflow==2.12.0 huggingface-hub==0.12.0 accelerate==0.15.0 transformers==4.26.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6 torchvision albumentations\n",
        "      !pip -q install -U tensorflow huggingface-hub==0.12.0 accelerate==0.15.0 transformers==4.26.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6 torchvision albumentations\n",
        "      if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "        clear_output()\n",
        "        step4a_installed_flag = True\n",
        "      else:\n",
        "        print(\"❌ Error installing dependencies, trying to continue anyway...\")\n",
        "    \"\"\"\n",
        "    print(\"\\n🚶‍♂️ Launching program...\\n\")\n",
        "\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "    %env PYTHONPATH={kohya}\n",
        "    !python {kohya}/finetune/tag_images_by_wd14_tagger.py \\\n",
        "      {images_folder} \\\n",
        "      --repo_id=SmilingWolf/wd-v1-4-swinv2-tagger-v2 \\\n",
        "      --model_dir={root_dir} \\\n",
        "      --thresh={tag_threshold} \\\n",
        "      --batch_size=8 \\\n",
        "      --caption_extension=.txt \\\n",
        "      --force_download\n",
        "\n",
        "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "      print(\"removing underscores and blacklist...\")\n",
        "      blacklisted_tags = [t.strip() for t in blacklist_tags.split(\",\")]\n",
        "      print(\"Processing 2nd Blacklist...\")\n",
        "      blacklist_tags_2 = [t.strip() for t in blacklist_tags_2.split(\",\")]\n",
        "      combined_Blacklist = blacklisted_tags + blacklist_tags_2\n",
        "      print(\"Processing extraAbsorbedTags...\")\n",
        "      extraAbsorbedTags = [t.strip() for t in extraAbsorbedTags.split(\",\")]\n",
        "      from collections import Counter\n",
        "      top_tags = Counter()\n",
        "      for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "        with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "          tags = [t.strip() for t in f.read().split(\",\")]\n",
        "          tags = [t.replace(\"_\", \" \") if len(t) > 3 else t for t in tags]\n",
        "          tags = [t for t in tags if t not in blacklisted_tags]\n",
        "        top_tags.update(tags)\n",
        "        with open(os.path.join(images_folder, txt), 'w') as f:\n",
        "          f.write(\", \".join(tags))\n",
        "\n",
        "\n",
        "      %env PYTHONPATH=/env/python\n",
        "      #clear_output()\n",
        "      ### Original tagging output message\n",
        "      #print(f\"📊 Tagging complete. Here are the top 50 tags in your dataset:\")\n",
        "      #print(\"\\n\".join(f\"{k},\" for k, v in top_tags.most_common(50)))\n",
        "\n",
        "      ### Top 10 Tags Code\n",
        "      outputTags = [k for k, v in top_tags.most_common(50) if k not in blacklist_tags_2]\n",
        "      top10Tags = outputTags[:numberOfTopTagsToAbsorb]\n",
        "      tagsToAbsorb = top10Tags + extraAbsorbedTags\n",
        "      ### Debugging line\n",
        "      #print(f\"-----\\ntop10Tags = {top10Tags}\")\n",
        "      print(f\"-----\\ntagsToAbsorb = {tagsToAbsorb}\")\n",
        "      remove_tags = (\" \".join(f\"{y},\" for y in tagsToAbsorb))\n",
        "      print(f\"-----\\nremove_tags = {remove_tags}\\n\")\n",
        "\n",
        "      ### New Tagging to only show non-blacklist\n",
        "      print(f\"Currated tagging complete. Here are the top 50 tags after purging items from blacklist 2:\")\n",
        "      print(\"\\n\".join(f\"{k},\" for k, v in top_tags.most_common(50) if k not in blacklist_tags_2))\n",
        "\n",
        "\n",
        "\n",
        "  else: # Photos\n",
        "    if \"step4b_installed_flag\" not in globals():\n",
        "      print(\"\\n🏭 Installing dependencies...\\n\")\n",
        "      #!pip -q install timm==0.6.12 fairscale==0.4.13 transformers==4.26.0 requests==2.28.2 accelerate==0.15.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6\n",
        "      !pip -q install -U timm==0.6.12 fairscale==0.4.13 transformers==4.26.0 requests==2.28.2 accelerate==0.15.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6\n",
        "      if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "        clear_output()\n",
        "        step4b_installed_flag = True\n",
        "      else:\n",
        "        print(\"❌ Error installing dependencies, trying to continue anyway...\")\n",
        "\n",
        "    print(\"\\n🚶‍♂️ Launching program...\\n\")\n",
        "\n",
        "    os.chdir(kohya)\n",
        "    %env PYTHONPATH={kohya}\n",
        "    !python {kohya}/finetune/make_captions.py \\\n",
        "      {images_folder} \\\n",
        "      --beam_search \\\n",
        "      --max_data_loader_n_workers=2 \\\n",
        "      --batch_size=8 \\\n",
        "      --min_length={caption_min} \\\n",
        "      --max_length={caption_max} \\\n",
        "      --caption_extension=.txt\n",
        "\n",
        "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "      import random\n",
        "      captions = [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]\n",
        "      sample = []\n",
        "      for txt in random.sample(captions, min(10, len(captions))):\n",
        "        with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "          sample.append(f.read())\n",
        "\n",
        "      os.chdir(root_dir)\n",
        "      %env PYTHONPATH=/env/python\n",
        "      clear_output()\n",
        "      print(f\"📊 Captioning complete. Here are {len(sample)} example captions from your dataset:\")\n",
        "      print(\"\".join(sample))\n",
        "\n",
        "    end_time_tagging = time.perf_counter()\n",
        "    time_total_tagging = end_time_tagging-start_time_tagging\n",
        "    print(f\"Tagging took {(time_total_tagging/60):0.1f} minutes ({time_total_tagging:0.1f} seconds)\")\n",
        "\n",
        "def print_a_line():\n",
        "  print(\"========================================================================================================================\")\n",
        "\n",
        "def print_important_log (logMessage):\n",
        "  print(\"========================================================================================================================\")\n",
        "  print(logMessage)\n",
        "  print(\"========================================================================================================================\")\n",
        "\n",
        "\n",
        "#######################################################\n",
        "##### STEP - Curate tags!\n",
        "#######################################################\n",
        "print(\"#######################################################\")\n",
        "print(\"##### STEP - Curate tags!\")\n",
        "print(\"#######################################################\")\n",
        "\n",
        "search_tags = \"\"\n",
        "replace_with = \"\"\n",
        "search_mode = \"OR\"\n",
        "new_becomes_activation_tag = False\n",
        "sort_alphabetically = False\n",
        "remove_duplicates = False\n",
        "\n",
        "def split_tags(tagstr):\n",
        "  return [s.strip() for s in tagstr.split(\",\") if s.strip()]\n",
        "\n",
        "activation_tag_list = split_tags(global_activation_tag)\n",
        "remove_tags_list = split_tags(remove_tags)\n",
        "search_tags_list = split_tags(search_tags)\n",
        "replace_with_list = split_tags(replace_with)\n",
        "replace_new_list = [t for t in replace_with_list if t not in search_tags_list]\n",
        "\n",
        "replace_with_list = [t for t in replace_with_list if t not in replace_new_list]\n",
        "replace_new_list.reverse()\n",
        "activation_tag_list.reverse()\n",
        "\n",
        "remove_count = 0\n",
        "replace_count = 0\n",
        "\n",
        "for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "\n",
        "  with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "    tags = [s.strip() for s in f.read().split(\",\")]\n",
        "\n",
        "  if remove_duplicates:\n",
        "    tags = list(set(tags))\n",
        "  if sort_alphabetically:\n",
        "    tags.sort()\n",
        "\n",
        "  for rem in remove_tags_list:\n",
        "    if rem in tags:\n",
        "      remove_count += 1\n",
        "      tags.remove(rem)\n",
        "\n",
        "  if \"AND\" in search_mode and all(r in tags for r in search_tags_list) \\\n",
        "      or \"OR\" in search_mode and any(r in tags for r in search_tags_list):\n",
        "    replace_count += 1\n",
        "    for rem in search_tags_list:\n",
        "      if rem in tags:\n",
        "        tags.remove(rem)\n",
        "    for add in replace_with_list:\n",
        "      if add not in tags:\n",
        "        tags.append(add)\n",
        "    for new in replace_new_list:\n",
        "      if new_becomes_activation_tag:\n",
        "        if new in tags:\n",
        "          tags.remove(new)\n",
        "        tags.insert(0, new)\n",
        "      else:\n",
        "        if new not in tags:\n",
        "          tags.append(new)\n",
        "\n",
        "  for act in activation_tag_list:\n",
        "    if act in tags:\n",
        "      tags.remove(act)\n",
        "    tags.insert(0, act)\n",
        "\n",
        "  with open(os.path.join(images_folder, txt), 'w') as f:\n",
        "    f.write(\", \".join(tags))\n",
        "\n",
        "if global_activation_tag:\n",
        "  print(f\"\\n📎 Applied new activation tag(s): {', '.join(activation_tag_list)}\")\n",
        "if remove_tags:\n",
        "  print(f\"\\n🚮 Removed {remove_count} tags.\")\n",
        "if search_tags:\n",
        "  print(f\"\\n💫 Replaced in {replace_count} files.\")\n",
        "\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "time_total = end_time-start_time\n",
        "print(f\"\\n✅ Done! Process took {(time_total/60):0.1f} minutes ({time_total:0.1f} seconds)\")\n",
        "\n",
        "\n",
        "#######################################################\n",
        "##### Create log file - Tagging\n",
        "#######################################################\n",
        "\n",
        "\n",
        "#Create log file\n",
        "directory = main_dir +\"/log\"\n",
        "dateTimeFormatedForFilename = colabUtilities.getDateTimeFormatedForFilename()\n",
        "logFileName = directory + \"/\" + project_name + \"_\" + dateTimeFormatedForFilename + \".log\"\n",
        "\n",
        "\n",
        "\n",
        "#Write all the top tags to file\n",
        "if (tag_images):\n",
        "\n",
        "  #Write the top 50 tags found in the images to file\n",
        "  linuxSafeFormattedTop50Tags = colabUtilities.reformatToSafeString(str(\" \".join(f\"{logTags_i},\" for logTags_i, v in top_tags.most_common(50))))\n",
        "  topTags = linuxSafeFormattedTop50Tags\n",
        "\n",
        "fileName = project_name + \"_\" + dateTimeFormatedForFilename + \".log\"\n",
        "trigger =  str(\" \".join(f\"{trigger_i},\" for trigger_i in activation_tag_list))\n",
        "removedTags = remove_tags\n",
        "taggingTime = time_total\n",
        "\n",
        "\n",
        "if (create_logs):\n",
        "  colabUtilities.writeLogHeaderToFile(directory, fileName, project_name)\n",
        "  colabUtilities.writeLogForTagging(directory,\n",
        "                          fileName,\n",
        "                          trigger,\n",
        "                          gelbooruSearchQuery,\n",
        "                          removedTags,\n",
        "                          topTags,\n",
        "                          taggingTime)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "SX0PF-iceRJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Run this Cell to Disconnect from the Runtime\n",
        "#@markdown This is useful if you have a long running process and you want to disconnect once its done. It helps you not waste your free GPU time limit.\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rPo40aiqwd7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Duplicate a folder\n",
        "#@markdown Use this if you want to make multiple projects with the same training data. This is hardcoded to get datasets from Google Drive\n",
        "main_dir      = os.path.join(\"/content\", \"drive/MyDrive/lora_training\")\n",
        "\n",
        "local_var_working_dir = os.path.join(main_dir, \"datasets\")\n",
        "folder_to_duplicate = \"\" #@param {type:\"string\"}\n",
        "duplicate_folder_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "%ls\n",
        "%cp -av {local_var_working_dir}/{folder_to_duplicate} {local_var_working_dir}/{duplicate_folder_name}\n"
      ],
      "metadata": {
        "id": "MnnpS58alGzu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check if the folder exists\n",
        "import os\n",
        "#@markdown Use this to make sure your dataset is where it should be.\n",
        "project_name = \"\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "root_dir = \"/content\"\n",
        "main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\")\n",
        "images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "if(not os.path.exists(images_folder)):\n",
        "  print(\"Error Folder does not exist\")\n",
        "else:\n",
        "  print(f\"Number of images in folder is {colabUtilities.countNumberOfImagesInFolder(images_folder)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kr4XCFj0Gvw0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}