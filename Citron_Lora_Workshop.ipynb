{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simplified Lora Workshop by Citron Legacy üçã\n",
        "\n",
        "Help fuel my passion!\n",
        " [![ko-fi](https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/citronlegacy)\n",
        "\n",
        "Any amount would be awesome!\n",
        "\n",
        "![](https://i.imgur.com/sjXiQwT.png)\n",
        "\n",
        "\n",
        "### Project Description\n",
        "\n",
        "This project is for simplying the training of Loras for Stable Diffusion. There are 2 steps\n",
        "1. Make a Dataset\n",
        "2. Make a Lora\n",
        "\n",
        "There are a lot of great Lora training tools with nice features but this one is intended to hide advanced settings and make the simplest trainer possible.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Links\n",
        "| Project |GitHub| Colab | | Other content | Link|\n",
        "|:--|:-:|:-:|:-:|:--|:--|\n",
        "| üè† **Homepage** | [![GitHub](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/github.svg)](https://github.com/citronlegacy/kohya-colab) | | | ‚òï **Ko-fi** | [![Ko-Fi](https://img.shields.io/badge/Ko--Fi-Support-orange.svg)](https://ko-fi.com/citronlegacy) |\n",
        "| üõ†Ô∏è **Citron Lora Workshop (Dataset & Training)** | [![GitHub](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/github.svg)](https://github.com/citronlegacy/kohya-colab/blob/main/Citron_Lora_Workshop.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/citronlegacy/kohya-colab/blob/main/Citron_Lora_Workshop.ipynb) | |ü§ñ **CivitAI** | [![CivitAI](https://img.shields.io/badge/CivitAI-Models-blue.svg)](https://civitai.com/user/CitronLegacy/models) |\n",
        "| üí™ **Citron Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/github.svg)](https://github.com/citronlegacy/kohya-colab/blob/main/Citron_Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/citronlegacy/kohya-colab/blob/main/Citron_Lora_Trainer.ipynb) | | üé® **Pixiv** | [![Pixiv](https://img.shields.io/badge/Pixiv-Profile-purple.svg)](https://www.pixiv.net/en/users/95364318) |\n",
        "| ‚≠ê **Coming soon!! CALM - Citron Auto Lora Maker** |  |  | | üé¨ **Youtube**  | [![YouTube](https://img.shields.io/badge/YouTube-Subscribe-red.svg)](https://www.youtube.com/@FujiwaraNoMokou11) |\n",
        "| üìä **Citron Dataset Maker** | [![GitHub](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/github.svg)](https://github.com/citronlegacy/kohya-colab/blob/main/Citron_Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/citronlegacy/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/citronlegacy/kohya-colab/blob/main/Citron_Dataset_Maker.ipynb) | | | |\n",
        "\n",
        "\n",
        "---\n",
        "### Project Disclaimer\n",
        "This is forked from the work of [Hollowstrawberry üçì](https://github.com/hollowstrawberry/kohya-colab) which is based on the work of [Kohya-ss](https://github.com/kohya-ss/sd-scripts) and [Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb). Thank you!\n",
        "\n",
        "Please read and follow the [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) and its [Terms of Service](https://research.google.com/colaboratory/tos_v3.html).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HCOeLF9-2CbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Install\n",
        "#@markdown ### 1Ô∏è‚É£ Setup Connect to Google Drive and Install Dependences\n",
        "#@markdown Installation usually takes about 3 minutes\n",
        "\n",
        "#@markdown ------------------------------------------------------\n",
        "\n",
        "#@markdown Update 02/03/2024 - First release of my Citron Lora Workshop\n",
        "\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from google.colab.output import clear as clear_output\n",
        "from google.colab import drive\n",
        "import re\n",
        "import toml\n",
        "import shutil\n",
        "import zipfile\n",
        "from IPython.display import Markdown, display\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"üìÇ Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "COLAB = True # low ram\n",
        "COMMIT = \"e6ad3cbc66130fdc3bf9ecd1e0272969b1d613f7\"\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def count_images_in_folder(folder_path):\n",
        "    # Ensure the folder path is a valid directory\n",
        "    folder_path = Path(folder_path)\n",
        "    if not folder_path.is_dir():\n",
        "        raise ValueError(f\"The provided path '{folder_path}' does not exist. If it does exist but Colab can't find it try reconnecting to Google Drive\")\n",
        "\n",
        "    # Count the number of image files in the folder\n",
        "    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp']\n",
        "    image_count = sum(1 for file in folder_path.iterdir() if file.suffix.lower() in image_extensions)\n",
        "\n",
        "    return image_count\n",
        "\n",
        "\n",
        "def clone_repo():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone https://github.com/kohya-ss/sd-scripts {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/requirements.txt -q -O requirements.txt\n",
        "\n",
        "def install_dependencies():\n",
        "  clone_repo()\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "  !pip install --upgrade -r requirements.txt\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if COLAB:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py # low ram\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "  global dependencies_installed\n",
        "  dependencies_installed = True\n",
        "\n",
        "aiInstalls_start_time = time.perf_counter()\n",
        "install_dependencies()\n",
        "aiInstalls_end_time = time.perf_counter()\n",
        "\n",
        "\n",
        "#######################################################\n",
        "##### Start - Define Citron's Library Fuctions\n",
        "#######################################################\n",
        "\n",
        "def writeToFile(filename, text):\n",
        "  ! echo {text} >> {filename}\n",
        "  #! cat {filename}\n",
        "\n",
        "def clearFile(filename):\n",
        "   ! echo \"\" > {filename}\n",
        "\n",
        "def writeLineToFile(filename):\n",
        "  ! echo \"==============================\" >> {filename}\n",
        "\n",
        "#######################################################\n",
        "##### End - Define Citron's Library Fuctions\n",
        "#######################################################\n",
        "\n",
        "#Import colabUtilities\n",
        "!git clone https://github.com/citronlegacy/kohya-colab.git\n",
        "# CD into project\n",
        "%cd kohya-colab\n",
        "# import modules\n",
        "import colabUtilities\n",
        "#return to original directory\n",
        "%cd ..\n",
        "\n",
        "print(\"AI Installation time: \" + str(colabUtilities.get_time_hh_mm_ss(aiInstalls_end_time-aiInstalls_start_time)) + \" minutes\")"
      ],
      "metadata": {
        "id": "x_xaxQL57ImS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nBUO636P85t",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Dataset Maker\n",
        "\n",
        "import os\n",
        "import time\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display, Markdown\n",
        "import json\n",
        "from urllib.request import urlopen, Request\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "COLAB = True\n",
        "\n",
        "if COLAB:\n",
        "  from google.colab.output import clear as clear_output\n",
        "else:\n",
        "  from IPython.display import clear_output\n",
        "\n",
        "#defining variables at the beginning to fix a bug with the log file - lol such programming\n",
        "remove_tags = \"NA - Tagging Skipped\"\n",
        "topTags = \"NA - Tagging Skipped\"\n",
        "total_steps = 0 #Defining this variable here so that its a global varaible\n",
        "\n",
        "\n",
        "#@markdown üí° Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): Dataset creation is the most important part of Lora training. Take your time and have fun collecting images of something you like.\n",
        "#@markdown This project has a lot of tips but feel free to ignore them! You are the creator, don't let anything restrict your creativity! üéâ\n",
        "#@markdown ### 1Ô∏è‚É£ Setup\n",
        "download_images = True #@param {type:\"boolean\"}\n",
        "tag_images = True #@param {type:\"boolean\"}\n",
        "create_logs = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Your project name can't contain spaces\n",
        "project_name = \"Hatsune_Miku\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "#@markdown Folder Structure is Organized by project name: MyDrive/lora_training/datasets/project_name\n",
        "folder_structure = \"Organize by category (MyDrive/lora_training/datasets/project_name)\"\n",
        "\n",
        "if not project_name or any(c in project_name for c in \" .()\\\"'\\\\\") or project_name.count(\"/\") > 1:\n",
        "  print(\"Please write a valid project_name.\")\n",
        "\n",
        "project_base = project_name if \"/\" not in project_name else project_name[:project_name.rfind(\"/\")]\n",
        "project_subfolder = project_name if \"/\" not in project_name else project_name[project_name.rfind(\"/\")+1:]\n",
        "\n",
        "\n",
        "main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "\n",
        "for dir in [main_dir, deps_dir, images_folder, config_folder]:\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Project {project_name} is ready!\")\n",
        "\n",
        "\n",
        "#######################################################\n",
        "##### STEP - Image Downloading\n",
        "#######################################################\n",
        "print(\"#######################################################\")\n",
        "print(\"##### STEP - Image Downloading\")\n",
        "print(\"#######################################################\")\n",
        "\n",
        "if (not download_images):\n",
        "  print(\"skipping image download\")\n",
        "  gelbooruSearchQuery = \"NA step is skipped\" #setting this value because this step is skipped\n",
        "if (download_images):\n",
        "\n",
        "  #@markdown ### 2Ô∏è‚É£ Scrape images from Gelbooru\n",
        "\n",
        "  #@markdown We will grab images from the popular anime gallery [Gelbooru](https://gelbooru.com/). Images are sorted by tags, including poses, scenes, character traits, character names, artists, etc. <p>\n",
        "  #@markdown * If you instead want to download screencaps of anime episodes, try [this other colab by another person](https://colab.research.google.com/drive/1oBSntB40BKzNmKceXUlkXzujzdQw-Ci7). It's more complicated though.\n",
        "\n",
        "  #@markdown Up to 1000 images may be downloaded by this step in just one minute. Don't abuse it. <p>\n",
        "\n",
        "  #@markdown Note: putting a minus sign in front of a tag will exclude images with that tag from the search result. For example, `-pink_hair` will mean you won't get images with the `pink_hair` tag\n",
        "\n",
        "  #@markdown üí° Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): If you want to be really detailed you can run download step several times with different search queries.\n",
        "  #@markdown For example, get a few images with the `from_behind` or `from_side` tag so that the Lora can learn those angles/concepts.\n",
        "\n",
        "  tags = \"Hatsune_Miku\" #@param {type:\"string\"}\n",
        "  #@markdown You don't have to use this but sometimes it nice to define tags that never change in this seperate input field. For example I always want search results sorted by score and I never want images with certain tags\n",
        "  extra_tags = \"sort:score solo -animated -crying -1boy -monochrome -duskyer -rakko_(r2) -slave -injury -scared -futanari -geebomb -bokuman -1340smile -osg_pk -2girls -among_us -rope -bdsm -feet \" #@param {type:\"string\"}\n",
        "\n",
        "  #@markdown the tag `rating:general` is basically the SFW tag on Gelbooru. You can use these these checkboxes to either limit results to SFW images or exclude SFW images from results.\n",
        "\n",
        "  #@markdown üí° Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): I recommend having at least 75% SFW images if you want to train an outfit. Too many NSFW images may result in a lack of data about clothes.\n",
        "  apply_tag_rating_general = False #@param {type:\"boolean\"}\n",
        "  apply_tag_minus_rating_general = False #@param {type:\"boolean\"}\n",
        "  if (apply_tag_rating_general):\n",
        "    tags = \"rating:general \" + tags\n",
        "  if (apply_tag_minus_rating_general):\n",
        "     tags = \"-rating:general \" + tags\n",
        "\n",
        "  tags = tags + \" \" + extra_tags\n",
        "  gelbooruSearchQuery = tags\n",
        "  gelbooruSearchQuery = gelbooruSearchQuery.replace(\"(\", \"\\(\").replace(\")\", \"\\)\")\n",
        "\n",
        "  ##@markdown If an image is bigger than this resolution a smaller version will be downloaded instead.\n",
        "  max_resolution = 3072 #param {type:\"slider\", min:1024, max:8196, step:1024}\n",
        "  ##@markdown Posts with a parent post are often minor variations of the same image.\n",
        "  include_posts_with_parent = True #param {type:\"boolean\"}\n",
        "\n",
        "  tags = tags.replace(\" \", \"+\")\\\n",
        "            .replace(\"(\", \"%28\")\\\n",
        "            .replace(\")\", \"%29\")\\\n",
        "            .replace(\":\", \"%3a\")\\\n",
        "            .replace(\"&\", \"%26\")\\\n",
        "\n",
        "  url = \"https://gelbooru.com/index.php?page=dapi&json=1&s=post&q=index&limit=100&tags={}\".format(tags)\n",
        "  user_agent = \"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; Googlebot/2.1; +http://www.google.com/bot.html) Chrome/93.0.4577.83 Safari/537.36\"\n",
        "  limit = 100 # hardcoded by gelbooru\n",
        "  #@markdown Enter maximum number of images to download from Gelbooru (There is a bug where it sometimes downloads 1 more/less than the number entered)\n",
        "\n",
        "  #@markdown üí° Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): You can make a decent Lora with 50 to 100 images. 300+ images is great but can take a really long time to train.\n",
        "  maxNumberOfImages = 50 #@param {type:\"number\"}\n",
        "  total_limit = maxNumberOfImages\n",
        "  # Testing setting the limit of images at this point in the code. If this works then the line where the url is set above can be deleted\n",
        "  #urlwithLimit = \"https://gelbooru.com/index.php?page=dapi&json=1&s=post&q=index&limit=\"+str(maxNumberOfImages)+\"&tags={}\".format(tags)\n",
        "  #url = urlwithLimit.format(tags)\n",
        "  #if (maxNumberOfImages < 100):\n",
        "  #  url = \"https://gelbooru.com/index.php?page=dapi&json=1&s=post&q=index&limit=\"+str(maxNumberOfImages)+\"&tags={}\".format(tags)\n",
        "\n",
        "  #url = \"https://gelbooru.com/index.php?page=dapi&json=1&s=post&q=index&limit=\"+str(maxNumberOfImages)+\"&tags={}\".format(tags)\n",
        "\n",
        "  print(url)\n",
        "\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\")\n",
        "\n",
        "  \"\"\"\n",
        "  def ubuntu_deps():\n",
        "    print(\"üè≠ Installing dependencies...\\n\")\n",
        "    !apt -y install aria2\n",
        "    return not get_ipython().__dict__['user_ns']['_exit_code']\n",
        "\n",
        "  if \"step2_installed_flag\" not in globals():\n",
        "    if ubuntu_deps():\n",
        "      #clear_output()\n",
        "      step2_installed_flag = True\n",
        "    else:\n",
        "      print(\"‚ùå Error installing dependencies, attempting to continue anyway...\")\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def get_json(url):\n",
        "    print(\"get_json url = \" + url)\n",
        "    with urlopen(Request(url, headers={\"User-Agent\": user_agent})) as page:\n",
        "      return json.load(page)\n",
        "\n",
        "  def filter_images(data):\n",
        "    return [p[\"file_url\"] if p[\"width\"]*p[\"height\"] <= max_resolution**2 else p[\"sample_url\"]\n",
        "            for p in data[\"post\"]\n",
        "            if (p[\"parent_id\"] == 0 or include_posts_with_parent)\n",
        "            and p[\"file_url\"].lower().endswith(supported_types)]\n",
        "\n",
        "  def download_images():\n",
        "    count = 0\n",
        "    if(maxNumberOfImages < 100):\n",
        "      smallerThan100DownloadURL = \"https://gelbooru.com/index.php?page=dapi&json=1&s=post&q=index&limit=\"+str(maxNumberOfImages+1)+\"&tags={}\".format(tags)\n",
        "      data = get_json(smallerThan100DownloadURL)\n",
        "      count = data[\"@attributes\"][\"count\"]\n",
        "    else:\n",
        "      data = get_json(url)\n",
        "      count = data[\"@attributes\"][\"count\"]\n",
        "\n",
        "    if count == 0:\n",
        "      print(\"üì∑ No results found\")\n",
        "      return\n",
        "\n",
        "    print(f\"üéØ Found {count} results\")\n",
        "    test_url = \"https://gelbooru.com/index.php?page=post&s=list&tags={}\".format(tags)\n",
        "    display(Markdown(f\"[Click here to open in browser!]({test_url})\"))\n",
        "    print (f\"üîΩ Will download to {images_folder.replace('/content/drive/', '')} (A confirmation box should appear below, otherwise run this cell again)\")\n",
        "    inp = 'yes' #input(\"‚ùì Enter the word 'yes' if you want to proceed with the download: \")\n",
        "\n",
        "    if inp.lower().strip() != 'yes':\n",
        "      print(\"‚ùå Download cancelled\")\n",
        "      return\n",
        "\n",
        "    print(\"üì© Grabbing image list...\")\n",
        "\n",
        "    image_urls = set()\n",
        "    image_urls = image_urls.union(filter_images(data))\n",
        "    for i in range(total_limit // limit):\n",
        "      print(\"inside loop\")\n",
        "      numberOfImagesDownloadLinksInFile = len(image_urls)\n",
        "      numberOfDownloadsRemaining = maxNumberOfImages - numberOfImagesDownloadLinksInFile\n",
        "      #Debugging log - can be deleted\n",
        "      print (f\"i = {i}; image_urls = {len(image_urls)}; total_limit = {total_limit}; limit = {limit}; numberOfImagesDownloadLinksInFile = {numberOfImagesDownloadLinksInFile}; numberOfDownloadsRemaining = {numberOfDownloadsRemaining} \" )\n",
        "      count -= limit\n",
        "      if count <= 0:\n",
        "        break\n",
        "      time.sleep(0.1)\n",
        "\n",
        "      # Reformat URLs to ensure only the correct amount of images is downloaded\n",
        "      #Added if-block to ensure that the last set of images to download doesnt go over the maxNumberOfImages\n",
        "      #Determine how many download links are remaining\n",
        "      #If there are less than the limit (hardcoded to 100) then only add the right amount of urls to the downloads list\n",
        "      numberOfDownloadsRemaining = maxNumberOfImages - numberOfImagesDownloadLinksInFile\n",
        "      if (numberOfDownloadsRemaining < 100):\n",
        "        filterImagesResult = filter_images(get_json(url+f\"&pid={i+1}\"))\n",
        "        limitedFilterImagesResult = filterImagesResult[0:numberOfDownloadsRemaining]\n",
        "        image_urls = image_urls.union(limitedFilterImagesResult)\n",
        "      else:\n",
        "        image_urls = image_urls.union(filter_images(get_json(url+f\"&pid={i+1}\")))\n",
        "\n",
        "    scrape_file = os.path.join(config_folder, f\"scrape_{project_subfolder}.txt\")\n",
        "    with open(scrape_file, \"w\") as f:\n",
        "      f.write(\"\\n\".join(image_urls))\n",
        "\n",
        "    print(f\"üåê Saved links to {scrape_file}\\n\\nüîÅ Downloading images...\\n\")\n",
        "    old_img_count = len([f for f in os.listdir(images_folder) if f.lower().endswith(supported_types)])\n",
        "\n",
        "    os.chdir(images_folder)\n",
        "    !aria2c --console-log-level=warn -c -x 16 -k 1M -s 16 -i {scrape_file}\n",
        "\n",
        "    new_img_count = len([f for f in os.listdir(images_folder) if f.lower().endswith(supported_types)])\n",
        "    print(f\"\\n‚úÖ Downloaded {new_img_count - old_img_count} images.\")\n",
        "    print(f\"\\n number of images in image_urls: {len(image_urls)} \")\n",
        "\n",
        "  download_images()\n",
        "  #clear_output()\n",
        "\n",
        "#######################################################\n",
        "##### STEP - Tagging\n",
        "#######################################################\n",
        "print(\"#######################################################\")\n",
        "print(\"##### STEP - Tagging\")\n",
        "print(\"#######################################################\")\n",
        "\n",
        "#@markdown ### 3Ô∏è‚É£ Tag your images\n",
        "#@markdown We will be using AI to automatically tag your images, specifically [Waifu Diffusion](https://huggingface.co/SmilingWolf/wd-v1-4-swinv2-tagger-v2) in the case of anime and [BLIP](https://huggingface.co/spaces/Salesforce/BLIP) in the case of photos.\n",
        "#@markdown Giving tags/captions to your images allows for much better training. This process should take a couple minutes. <p>\n",
        "\n",
        "#@markdown ‚ùó Important: you can choose to not enter anything in this section if you want to train your lora without a trigger\n",
        "\n",
        "trigger = \"Hatsune_Miku\" #@param {type:\"string\"}\n",
        "global_activation_tag = trigger.strip()\n",
        "if (not tag_images):\n",
        "  print(\"skipping image tagging\")\n",
        "if (tag_images):\n",
        "  start_time_tagging = time.perf_counter()\n",
        "  method = \"Anime tags\"\n",
        "  #@markdown Abosrb tags that represent your Lora. This could be details like eye color or concepts like `glowing`\n",
        "\n",
        "  #@markdown üí° Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): If you aren't sure what tags to absorb run this to see what tags are most common. You can check the logs and the rerun this with more absorbed tags\n",
        "\n",
        "  #@markdown üí° Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): For Character Loras, I recommend absorbing `1girl` or `1boy`, `solo`, eye color, hair color, and hair length/style (`short_hair`, `long_hair`, `twintails`, etc)\n",
        "\n",
        "  #@markdown üí° Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): For Character Loras, I recommend not absorbing details about an outfit. If you absorb the outfit into the trigger the Lora will not be flexible enough to change the outfit\n",
        "\n",
        "  absorbed_these_tags_into_trigger = \"1girl, solo, \" #@param {type:\"string\"}\n",
        "  #@markdown Change the tag threshold if you are not getting enough tags\n",
        "  tag_threshold = 0.4 #@param {type:\"number\"}\n",
        "  #@markdown These tags will be ignored and thus not added to the captions\n",
        "  blacklist_tags = \"bangs, breasts, multicolored hair, two-tone hair, gradient hair, virtual youtuber, official alternate costume, official alternate hairstyle, official alternate hair length, alternate costume, alternate hairstyle, alternate hair length, alternate hair color\" #@param {type:\"string\"}\n",
        "  blacklist_tags_2 = \"\"\n",
        "\n",
        "  numberOfTopTagsToAbsorb = 0\n",
        "\n",
        "  extraAbsorbedTags = absorbed_these_tags_into_trigger\n",
        "  caption_min = 15\n",
        "  caption_max = 75\n",
        "\n",
        "  %env PYTHONPATH=/env/python\n",
        "  os.chdir(root_dir)\n",
        "  kohya = \"/content/kohya-trainer\"\n",
        "  if not os.path.exists(kohya):\n",
        "    !git clone https://github.com/kohya-ss/sd-scripts {kohya}\n",
        "    os.chdir(kohya)\n",
        "    !git reset --hard 5050971ac687dca70ba0486a583d283e8ae324e2\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "  if \"tags\" in method:\n",
        "    \"\"\"\n",
        "    if \"step4a_installed_flag\" not in globals():\n",
        "      print(\"\\nüè≠ Installing dependencies...\\n\")\n",
        "      #!pip -q install tensorflow==2.12.0 huggingface-hub==0.12.0 accelerate==0.15.0 transformers==4.26.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6 torchvision albumentations\n",
        "      !pip -q install -U tensorflow huggingface-hub==0.12.0 accelerate==0.15.0 transformers==4.26.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6 torchvision albumentations\n",
        "      if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "        clear_output()\n",
        "        step4a_installed_flag = True\n",
        "      else:\n",
        "        print(\"‚ùå Error installing dependencies, trying to continue anyway...\")\n",
        "    \"\"\"\n",
        "    print(\"\\nüö∂‚Äç‚ôÇÔ∏è Launching program...\\n\")\n",
        "\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "    %env PYTHONPATH={kohya}\n",
        "    !python {kohya}/finetune/tag_images_by_wd14_tagger.py \\\n",
        "      {images_folder} \\\n",
        "      --repo_id=SmilingWolf/wd-v1-4-swinv2-tagger-v2 \\\n",
        "      --model_dir={root_dir} \\\n",
        "      --thresh={tag_threshold} \\\n",
        "      --batch_size=8 \\\n",
        "      --caption_extension=.txt \\\n",
        "      --force_download\n",
        "\n",
        "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "      print(\"removing underscores and blacklist...\")\n",
        "      blacklisted_tags = [t.strip() for t in blacklist_tags.split(\",\")]\n",
        "      print(\"Processing 2nd Blacklist...\")\n",
        "      blacklist_tags_2 = [t.strip() for t in blacklist_tags_2.split(\",\")]\n",
        "      combined_Blacklist = blacklisted_tags + blacklist_tags_2\n",
        "      print(\"Processing extraAbsorbedTags...\")\n",
        "      extraAbsorbedTags = [t.strip() for t in extraAbsorbedTags.split(\",\")]\n",
        "      from collections import Counter\n",
        "      top_tags = Counter()\n",
        "      for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "        with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "          tags = [t.strip() for t in f.read().split(\",\")]\n",
        "          tags = [t.replace(\"_\", \" \") if len(t) > 3 else t for t in tags]\n",
        "          tags = [t for t in tags if t not in blacklisted_tags]\n",
        "        top_tags.update(tags)\n",
        "        with open(os.path.join(images_folder, txt), 'w') as f:\n",
        "          f.write(\", \".join(tags))\n",
        "\n",
        "\n",
        "      %env PYTHONPATH=/env/python\n",
        "      #clear_output()\n",
        "      ### Original tagging output message\n",
        "      #print(f\"üìä Tagging complete. Here are the top 50 tags in your dataset:\")\n",
        "      #print(\"\\n\".join(f\"{k},\" for k, v in top_tags.most_common(50)))\n",
        "\n",
        "      ### Top 10 Tags Code\n",
        "      outputTags = [k for k, v in top_tags.most_common(50) if k not in blacklist_tags_2]\n",
        "      top10Tags = outputTags[:numberOfTopTagsToAbsorb]\n",
        "      tagsToAbsorb = top10Tags + extraAbsorbedTags\n",
        "      ### Debugging line\n",
        "      #print(f\"-----\\ntop10Tags = {top10Tags}\")\n",
        "      print(f\"-----\\ntagsToAbsorb = {tagsToAbsorb}\")\n",
        "      remove_tags = (\" \".join(f\"{y},\" for y in tagsToAbsorb))\n",
        "      print(f\"-----\\nremove_tags = {remove_tags}\\n\")\n",
        "\n",
        "      ### New Tagging to only show non-blacklist\n",
        "      print(f\"Currated tagging complete. Here are the top 50 tags after purging items from blacklist 2:\")\n",
        "      print(\"\\n\".join(f\"{k},\" for k, v in top_tags.most_common(50) if k not in blacklist_tags_2))\n",
        "\n",
        "\n",
        "\n",
        "  else: # Photos\n",
        "    if \"step4b_installed_flag\" not in globals():\n",
        "      print(\"\\nüè≠ Installing dependencies...\\n\")\n",
        "      #!pip -q install timm==0.6.12 fairscale==0.4.13 transformers==4.26.0 requests==2.28.2 accelerate==0.15.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6\n",
        "      !pip -q install -U timm==0.6.12 fairscale==0.4.13 transformers==4.26.0 requests==2.28.2 accelerate==0.15.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6\n",
        "      if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "        clear_output()\n",
        "        step4b_installed_flag = True\n",
        "      else:\n",
        "        print(\"‚ùå Error installing dependencies, trying to continue anyway...\")\n",
        "\n",
        "    print(\"\\nüö∂‚Äç‚ôÇÔ∏è Launching program...\\n\")\n",
        "\n",
        "    os.chdir(kohya)\n",
        "    %env PYTHONPATH={kohya}\n",
        "    !python {kohya}/finetune/make_captions.py \\\n",
        "      {images_folder} \\\n",
        "      --beam_search \\\n",
        "      --max_data_loader_n_workers=2 \\\n",
        "      --batch_size=8 \\\n",
        "      --min_length={caption_min} \\\n",
        "      --max_length={caption_max} \\\n",
        "      --caption_extension=.txt\n",
        "\n",
        "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "      import random\n",
        "      captions = [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]\n",
        "      sample = []\n",
        "      for txt in random.sample(captions, min(10, len(captions))):\n",
        "        with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "          sample.append(f.read())\n",
        "\n",
        "      os.chdir(root_dir)\n",
        "      %env PYTHONPATH=/env/python\n",
        "      clear_output()\n",
        "      print(f\"üìä Captioning complete. Here are {len(sample)} example captions from your dataset:\")\n",
        "      print(\"\".join(sample))\n",
        "\n",
        "    end_time_tagging = time.perf_counter()\n",
        "    time_total_tagging = end_time_tagging-start_time_tagging\n",
        "    print(f\"Tagging took {(time_total_tagging/60):0.1f} minutes ({time_total_tagging:0.1f} seconds)\")\n",
        "\n",
        "def print_a_line():\n",
        "  print(\"========================================================================================================================\")\n",
        "\n",
        "def print_important_log (logMessage):\n",
        "  print(\"========================================================================================================================\")\n",
        "  print(logMessage)\n",
        "  print(\"========================================================================================================================\")\n",
        "\n",
        "\n",
        "#######################################################\n",
        "##### STEP - Curate tags!\n",
        "#######################################################\n",
        "print(\"#######################################################\")\n",
        "print(\"##### STEP - Curate tags!\")\n",
        "print(\"#######################################################\")\n",
        "\n",
        "search_tags = \"\"\n",
        "replace_with = \"\"\n",
        "search_mode = \"OR\"\n",
        "new_becomes_activation_tag = False\n",
        "sort_alphabetically = False\n",
        "remove_duplicates = False\n",
        "\n",
        "def split_tags(tagstr):\n",
        "  return [s.strip() for s in tagstr.split(\",\") if s.strip()]\n",
        "\n",
        "activation_tag_list = split_tags(global_activation_tag)\n",
        "remove_tags_list = split_tags(remove_tags)\n",
        "search_tags_list = split_tags(search_tags)\n",
        "replace_with_list = split_tags(replace_with)\n",
        "replace_new_list = [t for t in replace_with_list if t not in search_tags_list]\n",
        "\n",
        "replace_with_list = [t for t in replace_with_list if t not in replace_new_list]\n",
        "replace_new_list.reverse()\n",
        "activation_tag_list.reverse()\n",
        "\n",
        "remove_count = 0\n",
        "replace_count = 0\n",
        "\n",
        "for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "\n",
        "  with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "    tags = [s.strip() for s in f.read().split(\",\")]\n",
        "\n",
        "  if remove_duplicates:\n",
        "    tags = list(set(tags))\n",
        "  if sort_alphabetically:\n",
        "    tags.sort()\n",
        "\n",
        "  for rem in remove_tags_list:\n",
        "    if rem in tags:\n",
        "      remove_count += 1\n",
        "      tags.remove(rem)\n",
        "\n",
        "  if \"AND\" in search_mode and all(r in tags for r in search_tags_list) \\\n",
        "      or \"OR\" in search_mode and any(r in tags for r in search_tags_list):\n",
        "    replace_count += 1\n",
        "    for rem in search_tags_list:\n",
        "      if rem in tags:\n",
        "        tags.remove(rem)\n",
        "    for add in replace_with_list:\n",
        "      if add not in tags:\n",
        "        tags.append(add)\n",
        "    for new in replace_new_list:\n",
        "      if new_becomes_activation_tag:\n",
        "        if new in tags:\n",
        "          tags.remove(new)\n",
        "        tags.insert(0, new)\n",
        "      else:\n",
        "        if new not in tags:\n",
        "          tags.append(new)\n",
        "\n",
        "  for act in activation_tag_list:\n",
        "    if act in tags:\n",
        "      tags.remove(act)\n",
        "    tags.insert(0, act)\n",
        "\n",
        "  with open(os.path.join(images_folder, txt), 'w') as f:\n",
        "    f.write(\", \".join(tags))\n",
        "\n",
        "if global_activation_tag:\n",
        "  print(f\"\\nüìé Applied new activation tag(s): {', '.join(activation_tag_list)}\")\n",
        "if remove_tags:\n",
        "  print(f\"\\nüöÆ Removed {remove_count} tags.\")\n",
        "if search_tags:\n",
        "  print(f\"\\nüí´ Replaced in {replace_count} files.\")\n",
        "\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "time_total = end_time-start_time\n",
        "print(f\"\\n‚úÖ Done! Process took {(time_total/60):0.1f} minutes ({time_total:0.1f} seconds)\")\n",
        "\n",
        "\n",
        "#######################################################\n",
        "##### Create log file - Tagging\n",
        "#######################################################\n",
        "\n",
        "\n",
        "#Create log file\n",
        "directory = main_dir +\"/log\"\n",
        "dateTimeFormatedForFilename = colabUtilities.getDateTimeFormatedForFilename()\n",
        "logFileName = directory + \"/\" + project_name + \"_\" + dateTimeFormatedForFilename + \".log\"\n",
        "\n",
        "\n",
        "\n",
        "#Write all the top tags to file\n",
        "if (tag_images):\n",
        "\n",
        "  #Write the top 50 tags found in the images to file\n",
        "  linuxSafeFormattedTop50Tags = colabUtilities.reformatToSafeString(str(\" \".join(f\"{logTags_i},\" for logTags_i, v in top_tags.most_common(50))))\n",
        "  topTags = linuxSafeFormattedTop50Tags\n",
        "\n",
        "fileName = project_name + \"_\" + dateTimeFormatedForFilename + \".log\"\n",
        "trigger =  str(\" \".join(f\"{trigger_i},\" for trigger_i in activation_tag_list))\n",
        "removedTags = remove_tags\n",
        "taggingTime = time_total\n",
        "\n",
        "\n",
        "if (create_logs):\n",
        "  colabUtilities.writeLogHeaderToFile(directory, fileName, project_name)\n",
        "  colabUtilities.writeLogForTagging(directory,\n",
        "                          fileName,\n",
        "                          trigger,\n",
        "                          gelbooruSearchQuery,\n",
        "                          removedTags,\n",
        "                          topTags,\n",
        "                          taggingTime)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Lora Trainer\n",
        "\n",
        "#@markdown ###Define your dataset location\n",
        "#@markdown Your project name will be the same as the folder containing your images. Spaces aren't allowed.\n",
        "project_name = \"Hatsune_Miku\" #@param {type:\"string\"}\n",
        "#@markdown Is your dataset in Google Drive or Gogole Colab? (This hasn't been fully tested with datasets in Colab)\n",
        "environment_location = \"Google Drive (/content/drive/MyDrive/)\" #@param [\"Google Drive (/content/drive/MyDrive/)\", \"Google Colab (/content/)\"]\n",
        "\n",
        "\n",
        "folder_structure = \"Organize by category (MyDrive/lora_training/datasets/project_name)\"\n",
        "\n",
        "#@markdown Decide the model that will be downloaded and used for training. You can also choose your own by pasting its download link.\n",
        "\n",
        "#@markdown üí° Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): Use AnyLora if you want to train an art style.\n",
        "training_model = \"Anime (animefull-final-pruned-fp16.safetensors)\" #@param [\"Anime (animefull-final-pruned-fp16.safetensors)\", \"AnyLora (AnyLoRA_noVae_fp16-pruned.ckpt)\", \"Stable Diffusion (sd-v1-5-pruned-noema-fp16.safetensors)\"]\n",
        "optional_custom_training_model_url = \"\" #@param {type:\"string\"}\n",
        "custom_model_is_based_on_sd2 = False #@param {type:\"boolean\"}\n",
        "\n",
        "if \"Drive\" in environment_location:\n",
        "    environment_path = \"/content/drive/MyDrive/\"\n",
        "\n",
        "elif \"Colab\" in environment_location:\n",
        "    environment_path = \"\"\n",
        "\n",
        "main_dir      = os.path.join(environment_path, \"lora_training\")\n",
        "images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "output_folder = os.path.join(main_dir, \"output\", project_name)\n",
        "config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "log_folder    = os.path.join(main_dir, \"log\")\n",
        "\n",
        "assert count_images_in_folder(images_folder) > 0, f\"Error: No images found in the specified folder: {images_folder}.\"\n",
        "\n",
        "global total_steps\n",
        "total_steps = \"\"\n",
        "\n",
        "if optional_custom_training_model_url:\n",
        "  model_url = optional_custom_training_model_url\n",
        "elif \"AnyLora\" in training_model:\n",
        "  model_url = \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_noVae_fp16-pruned.ckpt\"\n",
        "elif \"Anime\" in training_model:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n",
        "else:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n",
        "\n",
        "if \"AnyLora\" in training_model:\n",
        "  training_modelLogName = \"AnyLoRA_noVae_fp16-pruned\"\n",
        "elif \"Anime\" in training_model:\n",
        "  training_modelLogName = \"Animefull-final-pruned-fp16\"\n",
        "elif \"Stable Diffusion\" in training_model:\n",
        "  training_modelLogName = \"sd-v1-5-pruned-noema-fp16\"\n",
        "else:\n",
        "  training_modelLogName = \"other\"\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "\n",
        "# These may be set by other cells, some are legacy\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "if \"optimizer\" not in globals():\n",
        "  optimizer = \"AdamW8bit\"\n",
        "if \"optimizer_args\" not in globals():\n",
        "  optimizer_args = None\n",
        "if \"continue_from_lora\" not in globals():\n",
        "  continue_from_lora = \"\"\n",
        "if \"weighted_captions\" not in globals():\n",
        "  weighted_captions = False\n",
        "if \"adjust_tags\" not in globals():\n",
        "  adjust_tags = False\n",
        "if \"keep_tokens_weight\" not in globals():\n",
        "  keep_tokens_weight = 1.0\n",
        "\n",
        "COLAB = True # low ram\n",
        "XFORMERS = True\n",
        "COMMIT = \"9a67e0df390033a89f17e70df5131393692c2a55\"\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Processing\n",
        "#@markdown Resolution of 512 is standard for Stable Diffusion 1.5. Higher resolution training is much slower but can lead to better details. <p>\n",
        "#@markdown Images will be automatically scaled while training to produce the best results, so you don't need to crop or resize anything yourself.\n",
        "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
        "#@markdown This option will train your images both normally and flipped, for no extra cost, to learn more from them. Turn it on specially if you have less than 20 images. <p>\n",
        "#@markdown **Turn it off if you care about asymmetrical elements in your Lora**.\n",
        "flip_aug = False #@param {type:\"boolean\"}\n",
        "caption_extension = \".txt\"\n",
        "shuffle_tags = True\n",
        "shuffle_caption = shuffle_tags\n",
        "activation_tags = \"1\"\n",
        "keep_tokens = int(activation_tags)\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Steps - Your images will repeat this number of times during training.\n",
        "\n",
        "#@markdown üí° Tip from [Hollowstrawberry](https://github.com/hollowstrawberry/kohya-colab): I recommend that your images multiplied by their repeats is between 200 and 400.\n",
        "\n",
        "#@markdown üí° Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): Use 10 Repeats (or less if you have too many steps and training takes longer than 2 hours.)\n",
        "\n",
        "\n",
        "num_repeats = 10 #@param {type:\"number\"}\n",
        "#@markdown Choose how long you want to train for. A good starting point is around 10 epochs or around 2000 steps.<p>\n",
        "#@markdown One epoch is a number of steps equal to: your number of images multiplied by their repeats, divided by batch size. <p>\n",
        "\n",
        "#@markdown üí° Tip from [Citron Legacy](https://civitai.com/user/CitronLegacy/models): Always use 10 Epochs until you know what you are doing.\n",
        "\n",
        "preferred_unit = \"Epochs\" #@param [\"Epochs\", \"Steps\"]\n",
        "how_many = 10 #@param {type:\"number\"}\n",
        "max_train_epochs = how_many if preferred_unit == \"Epochs\" else None\n",
        "max_train_steps = how_many if preferred_unit == \"Steps\" else None\n",
        "#@markdown Saving more epochs will let you compare your Lora's progress better.\n",
        "save_every_n_epochs = 5 #@param {type:\"number\"}\n",
        "keep_only_last_n_epochs = 10 #@param {type:\"number\"}\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "#@markdown Increasing the batch size makes training faster, but may make learning worse. Recommended 2 or 3.\n",
        "train_batch_size = 2 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "\n",
        "unet_lr = 5e-4\n",
        "text_encoder_lr = 1e-4\n",
        "lr_scheduler = \"cosine_with_restarts\"\n",
        "lr_scheduler_number = 3\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "lr_warmup_ratio = 0.05\n",
        "lr_warmup_steps = 0\n",
        "min_snr_gamma = True\n",
        "min_snr_gamma_value = 5.0 if min_snr_gamma else None\n",
        "\n",
        "#@markdown Dim - More dim means larger Lora, it can hold more information but more isn't always better.\n",
        "lora_type = \"LoRA\"\n",
        "dim_to_use = \"Dim=32; Alpha=16\" #@param [\"Dim=16; Alpha=8\", \"Dim=32; Alpha=16\", \"Dim=64; Alpha=32\"]\n",
        "dim_to_use_text = \"\"\n",
        "\n",
        "\n",
        "network_dim = 32\n",
        "network_alpha = 16\n",
        "\n",
        "if (dim_to_use == \"Dim=16; Alpha=8\"):\n",
        "  network_dim = 16\n",
        "  network_alpha = 8\n",
        "  dim_to_use_text = \"Dim 16 and Alpha 8\"\n",
        "elif (dim_to_use == \"Dim=32; Alpha=16\"):\n",
        "  network_dim = 32\n",
        "  network_alpha = 16\n",
        "  dim_to_use_text = \"Dim 32 and Alpha 16\"\n",
        "elif (dim_to_use == \"Dim=64; Alpha=32\"):\n",
        "  network_dim = 64\n",
        "  network_alpha = 32\n",
        "  dim_to_use_text = \"Dim 64 and Alpha 32\"\n",
        "\n",
        "conv_dim = 8\n",
        "conv_alpha = 4\n",
        "\n",
        "network_module = \"networks.lora\"\n",
        "network_args = None\n",
        "if lora_type.lower() == \"locon\":\n",
        "  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n",
        "\n",
        "\n",
        "\n",
        "# üë©‚Äçüíª Cool code goes here\n",
        "\n",
        "if optimizer.lower() == \"prodigy\" or \"dadapt\" in optimizer.lower():\n",
        "  if override_values_for_dadapt_and_prodigy:\n",
        "    unet_lr = 0.5\n",
        "    text_encoder_lr = 0.5\n",
        "    lr_scheduler = \"constant_with_warmup\"\n",
        "    lr_warmup_ratio = 0.05\n",
        "    network_alpha = network_dim\n",
        "\n",
        "  if not optimizer_args:\n",
        "    optimizer_args = [\"decouple=True\",\"weight_decay=0.01\",\"betas=[0.9,0.999]\"]\n",
        "    if optimizer == \"Prodigy\":\n",
        "      optimizer_args.extend([\"d_coef=2\",\"use_bias_correction=True\"])\n",
        "      if lr_warmup_ratio > 0:\n",
        "        optimizer_args.append(\"safeguard_warmup=True\")\n",
        "      else:\n",
        "        optimizer_args.append(\"safeguard_warmup=False\")\n",
        "\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, keep_tokens_weight, weighted_captions, adjust_tags\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "  print(\"\\nüíø Checking dataset...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"üí• Error: Please choose a valid project name.\")\n",
        "    return\n",
        "\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
        "    except:\n",
        "      print(f\"üí• Error: Your custom dataset is invalid or contains an error! Please check the original template.\")\n",
        "      return\n",
        "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "    folders = datasets_dict.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "  else:\n",
        "    reg = []\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"üí• Error: The folder {folder.replace('/content/drive/', '')} doesn't exist.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"üí• Error: Your {folder.replace('/content/drive/', '')} folder is empty.\")\n",
        "      return\n",
        "  for f in files:\n",
        "    if not f.lower().endswith(\".txt\") and not f.lower().endswith(supported_types):\n",
        "      print(f\"üí• Error: Invalid file in dataset: \\\"{f}\\\". Aborting.\")\n",
        "      return\n",
        "\n",
        "  if not [txt for txt in files if txt.lower().endswith(\".txt\")]:\n",
        "    caption_extension = \"\"\n",
        "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"üí• Error: Invalid path to existing Lora. Example: /content/drive/MyDrive/Loras/example.safetensors\")\n",
        "    return\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  global total_steps\n",
        "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
        "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"üìÅ\"+folder.replace(\"/content/drive/\", \"\") + (\" (Regularization)\" if folder in reg else \"\"))\n",
        "    print(f\"üìà Found {img} images with {rep} repeats, equaling {img*rep} steps.\")\n",
        "  print(f\"üìâ Divide {pre_steps_per_epoch} steps by {train_batch_size} batch size to get {steps_per_epoch} steps per epoch.\")\n",
        "  if max_train_epochs:\n",
        "    print(f\"üîÆ There will be {max_train_epochs} epochs, for around {total_steps} total training steps.\")\n",
        "  else:\n",
        "    print(f\"üîÆ There will be {total_steps} steps, divided into {estimated_epochs} epochs and then some.\")\n",
        "\n",
        "  if total_steps > 20000:\n",
        "    print(\"üí• Error: Your total steps are too high. You probably made a mistake. Aborting...\")\n",
        "    return\n",
        "\n",
        "  if adjust_tags:\n",
        "    print(f\"\\nüìé Weighted tags: {'ON' if weighted_captions else 'OFF'}\")\n",
        "    if weighted_captions:\n",
        "      print(f\"üìé Will use {keep_tokens_weight} weight on {keep_tokens} activation tag(s)\")\n",
        "    print(\"üìé Adjusting tags...\")\n",
        "    adjust_weighted_tags(folders, keep_tokens, keep_tokens_weight, weighted_captions)\n",
        "\n",
        "  return True\n",
        "\n",
        "def adjust_weighted_tags(folders, keep_tokens: int, keep_tokens_weight: float, weighted_captions: bool):\n",
        "  weighted_tag = re.compile(r\"\\((.+?):[.\\d]+\\)(,|$)\")\n",
        "  for folder in folders:\n",
        "    for txt in [f for f in os.listdir(folder) if f.lower().endswith(\".txt\")]:\n",
        "      with open(os.path.join(folder, txt), 'r') as f:\n",
        "        content = f.read()\n",
        "      # reset previous changes\n",
        "      content = content.replace('\\\\', '')\n",
        "      content = weighted_tag.sub(r'\\1\\2', content)\n",
        "      if weighted_captions:\n",
        "        # re-apply changes\n",
        "        content = content.replace(r'(', r'\\(').replace(r')', r'\\)').replace(r':', r'\\:')\n",
        "        if keep_tokens_weight > 1:\n",
        "          tags = [s.strip() for s in content.split(\",\")]\n",
        "          for i in range(min(keep_tokens, len(tags))):\n",
        "            tags[i] = f'({tags[i]}:{keep_tokens_weight})'\n",
        "          content = \", \".join(tags)\n",
        "      with open(os.path.join(folder, txt), 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\n‚≠ï Using custom config file {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"additional_network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": True if text_encoder_lr == 0 else None,\n",
        "        \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
        "        \"optimizer_type\": optimizer,\n",
        "        \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"max_train_steps\": max_train_steps,\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"noise_offset\": None,\n",
        "        \"clip_skip\": 2,\n",
        "        \"min_snr_gamma\": min_snr_gamma_value,\n",
        "        \"weighted_captions\": weighted_captions,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": XFORMERS,\n",
        "        \"lowram\": COLAB,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"mixed_precision\": \"fp16\",\n",
        "        \"output_dir\": output_folder,\n",
        "        \"logging_dir\": log_folder,\n",
        "        \"output_name\": project_name,\n",
        "        \"log_prefix\": project_name,\n",
        "      },\n",
        "      \"model_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "        \"v2\": custom_model_is_based_on_sd2,\n",
        "        \"v_parameterization\": True if custom_model_is_based_on_sd2 else None,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "      },\n",
        "      \"dreambooth_arguments\": {\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "      },\n",
        "      \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "      },\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\nüìÑ Config saved to {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"‚≠ï Using custom dataset config file {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_caption,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": flip_aug,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "        \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None if caption_extension else project_name\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"üìÑ Dataset config saved to {dataset_config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file\n",
        "  real_model_url = model_url.strip()\n",
        "\n",
        "  if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "    model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "  else:\n",
        "    model_file = \"/content/downloaded_model.safetensors\"\n",
        "    if os.path.exists(model_file):\n",
        "      !rm \"{model_file}\"\n",
        "\n",
        "  if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
        "    real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "  elif m := re.search(r\"(?:https?://)?(?:www\\\\.)?civitai\\.com/models/([0-9]+)(/[A-Za-z0-9-_]+)?\", model_url):\n",
        "    if m.group(2):\n",
        "      model_file = f\"/content{m.group(2)}.safetensors\"\n",
        "    if m := re.search(r\"modelVersionId=([0-9]+)\", model_url):\n",
        "      real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "    else:\n",
        "      raise ValueError(\"optional_custom_training_model_url contains a civitai link, but the link doesn't include a modelVersionId. You can also right click the download button to copy the direct download link.\")\n",
        "\n",
        "  !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "  if model_file.lower().endswith(\".safetensors\"):\n",
        "    from safetensors.torch import load_file as load_safetensors\n",
        "    try:\n",
        "      test = load_safetensors(model_file)\n",
        "      del test\n",
        "    except Exception as e:\n",
        "      #if \"HeaderTooLarge\" in str(e):\n",
        "      new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "      !mv \"{model_file}\" \"{new_model_file}\"\n",
        "      model_file = new_model_file\n",
        "      print(f\"Renamed model to {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "  if model_file.lower().endswith(\".ckpt\"):\n",
        "    from torch import load as load_ckpt\n",
        "    try:\n",
        "      test = load_ckpt(model_file)\n",
        "      del test\n",
        "    except Exception as e:\n",
        "      return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"üìÇ Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  if not dependencies_installed:\n",
        "    print(\"\\nüè≠ Installing dependencies...\\n\")\n",
        "    t0 = time()\n",
        "    install_dependencies()\n",
        "    t1 = time()\n",
        "    dependencies_installed = True\n",
        "    print(f\"\\n‚úÖ Installation finished in {int(t1-t0)} seconds.\")\n",
        "  else:\n",
        "    print(\"\\n‚úÖ Dependencies already installed.\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\nüîÑ Downloading model...\")\n",
        "    if not download_model():\n",
        "      print(\"\\nüí• Error: The model you selected is invalid or corrupted, or couldn't be downloaded. You can use a civitai or huggingface link, or any direct download link.\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\nüîÑ Model already downloaded.\\n\")\n",
        "\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\n‚≠ê Starting trainer...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    display(Markdown(\"[Download your Lora from Google Drive](https://drive.google.com/drive/my-drive)\\n\"\n",
        "                     \"There will be several files, you should try the latest version (the file with the largest number next to it)\"))\n",
        "\n",
        "start_time_make_lora = time.perf_counter()\n",
        "main()\n",
        "\n",
        "end_time_make_lora = time.perf_counter()\n",
        "time_total_make_lora = end_time_make_lora - start_time_make_lora\n",
        "time_total_timedelta = timedelta(seconds=time_total_make_lora)\n",
        "\n",
        "# Extract hours, minutes, and seconds from the timedelta\n",
        "hours, remainder = divmod(time_total_timedelta.seconds, 3600)\n",
        "minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "# Format the time\n",
        "creation_time = f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
        "\n",
        "display(Markdown(f\"### ‚úÖ [{project_name}] is Done! Lora Creation Process took: {creation_time}\"))\n",
        "\n",
        "#Create Log file\n",
        "def create_log_text(project_name, log_file_path, image_count, model_name, flip_aug, num_repeats, unit, num_epochs_or_steps, batch_size, total_steps, resolution, network_dim, creation_time):\n",
        "    log_data = [\n",
        "        f\"Trained on: {image_count} images\",\n",
        "        f\"Training Model: {model_name}\",\n",
        "        f\"flip_aug: {flip_aug}\",\n",
        "        f\"Num of Repeats: {num_repeats}\",\n",
        "        f\"Unit is Epochs or Steps: {unit}\",\n",
        "        f\"Number of Epochs or Steps: {num_epochs_or_steps}\",\n",
        "        f\"Training Batch Size: {batch_size}\",\n",
        "        f\"Total Steps: {total_steps}\",\n",
        "        f\"Resolution: {resolution}\",\n",
        "        f\"Network Dim: {network_dim}\",\n",
        "        f\"Lora Creation Process took: {creation_time}\",\n",
        "    ]\n",
        "\n",
        "    # Create a unique log filename based on the current timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_filename = f\"{project_name}_{timestamp}.txt\"\n",
        "    log_path_and_name = os.path.join(log_file_path, log_filename)\n",
        "    # Write the data to the text file\n",
        "    with open(log_path_and_name, \"w\") as text_file:\n",
        "        text_file.write(\"\\n\".join(log_data))\n",
        "\n",
        "    return log_path_and_name\n",
        "\n",
        "\n",
        "image_count = count_images_in_folder(images_folder)\n",
        "unit = preferred_unit\n",
        "num_epochs_or_steps = how_many\n",
        "\n",
        "\n",
        "log_filename = create_log_text(project_name, log_folder, image_count, training_modelLogName, flip_aug, num_repeats, unit, num_epochs_or_steps, train_batch_size, total_steps, resolution, dim_to_use_text, creation_time)\n",
        "print(f\"Log file created: {log_filename}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qIfd_sx99NWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "SX0PF-iceRJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Run this Cell to Disconnect from the Runtime\n",
        "#@markdown This is useful if you have a long running process and you want to disconnect once its done. It helps you not waste your free GPU time limit.\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rPo40aiqwd7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Duplicate a folder\n",
        "#@markdown Use this if you want to make multiple projects with the same training data. This is hardcoded to get datasets from Google Drive\n",
        "main_dir      = os.path.join(\"/content\", \"drive/MyDrive/lora_training\")\n",
        "\n",
        "local_var_working_dir = os.path.join(main_dir, \"datasets\")\n",
        "folder_to_duplicate = \"\" #@param {type:\"string\"}\n",
        "duplicate_folder_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "%ls\n",
        "%cp -av {local_var_working_dir}/{folder_to_duplicate} {local_var_working_dir}/{duplicate_folder_name}\n"
      ],
      "metadata": {
        "id": "MnnpS58alGzu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check if the folder exists\n",
        "import os\n",
        "#@markdown Use this to make sure your dataset is where it should be.\n",
        "project_name = \"\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "root_dir = \"/content\"\n",
        "main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\")\n",
        "images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "if(not os.path.exists(images_folder)):\n",
        "  print(\"Error Folder does not exist\")\n",
        "else:\n",
        "  print(f\"Number of images in folder is {colabUtilities.countNumberOfImagesInFolder(images_folder)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kr4XCFj0Gvw0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}